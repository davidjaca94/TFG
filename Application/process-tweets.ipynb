{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dependencias\n",
    "\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import unidecode\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "from operator import itemgetter\n",
    "from difflib import SequenceMatcher\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types\n",
    "from google.oauth2 import service_account\n",
    "from google.protobuf.json_format import MessageToDict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones generales\n",
    "\n",
    "def get_config(key):\n",
    "    jsonfile = open('config.json').read()\n",
    "    config = json.loads(jsonfile)\n",
    "    if key in config:\n",
    "        return config[key]\n",
    "\n",
    "def read_json(filepath):\n",
    "    jsonfile = io.open(filepath, encoding='utf8').read()\n",
    "    return json.loads(jsonfile)\n",
    "\n",
    "def save_json(filepath, content):\n",
    "    path = os.path.dirname(filepath)\n",
    "    if path: os.makedirs(path, exist_ok=True)\n",
    "    jsonfile = io.open(filepath, 'w', encoding='utf8')\n",
    "    jsoncontent = json.dumps(content, ensure_ascii=False)\n",
    "    jsonfile.write(jsoncontent)\n",
    "    jsonfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones para trabajar con los tuits recopilados\n",
    "\n",
    "def list_users_obtained():\n",
    "    users = []\n",
    "    folders = glob(\"data\\\\tweets\\\\*\\\\\")\n",
    "    for folder in folders:\n",
    "        user = re.search('.*\\\\\\\\\\d*@(.*)\\\\\\\\', folder).groups()[0]\n",
    "        users.append(user)\n",
    "    return users\n",
    "\n",
    "def get_user_path(user_nick):\n",
    "    folders = glob(\"data\\\\tweets\\\\*@%s\\\\\" % user_nick)\n",
    "    if (len(folders) == 0):\n",
    "        return None\n",
    "    else:\n",
    "        return folders[0]\n",
    "\n",
    "def load_user_tweets(user_nick):\n",
    "    folder = get_user_path(user_nick)\n",
    "    if folder:\n",
    "        files = glob(folder+\"*.json\")\n",
    "        for file in files:\n",
    "            yield read_json(file) # Se carga en memoria conforme se itera sobre la llamada\n",
    "\n",
    "def count_user_tweets(user_nick):\n",
    "    folder = get_user_path(user_nick)\n",
    "    if folder:\n",
    "        files = glob(folder+\"*.json\")\n",
    "        return len(files)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def group_by_language(tweets):\n",
    "    language_dict = {}\n",
    "    for tweet in tweets:\n",
    "        lang = tweet['lang']\n",
    "        tweet_list = language_dict[lang] if (lang in language_dict) else []\n",
    "        tweet_list.append(tweet)\n",
    "        language_dict[lang] = tweet_list\n",
    "    return language_dict\n",
    "\n",
    "def reduce_languages(tweets_by_language, languages):\n",
    "    language_dict = {}\n",
    "    for lang in tweets_by_language:\n",
    "        if lang in languages:\n",
    "            language_dict[lang] = tweets_by_language[lang]\n",
    "        else:\n",
    "            other_lang = language_dict['others'] if 'others' in language_dict else []\n",
    "            other_lang.extend(tweets_by_language[lang])\n",
    "            language_dict['others'] = other_lang\n",
    "    return language_dict\n",
    "\n",
    "def compact_tweets(tweets):\n",
    "    text = \"\"\n",
    "    for tweet in tweets:\n",
    "        text += tweet['text'].replace('\\n', ' ') + '\\n'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones para trabajar con las entidades obtenidas de los tuits\n",
    "\n",
    "def path_entities(user):\n",
    "    return get_user_path(user).replace(\"tweets\", \"entities\")[:-1]\n",
    "\n",
    "def user_processed(user):\n",
    "    files = glob(path_entities(user)+\"*.json\")\n",
    "    return len(files) > 0\n",
    "\n",
    "def save_entities(entities, user, lang):\n",
    "    filepath = path_entities(user) + '#' + lang.upper() + '.json'\n",
    "    save_json(filepath, entities)\n",
    "\n",
    "def load_entities(user):\n",
    "    entities = []\n",
    "    files = glob(path_entities(user)+\"*.json\")\n",
    "    for file in files:\n",
    "        content = read_json(file)\n",
    "        if ('entities' in content):\n",
    "            entities.extend(content['entities'])\n",
    "    return entities\n",
    "\n",
    "def relevant_entity(entity):\n",
    "    entity = unidecode.unidecode(entity.lower())\n",
    "    black_list = io.open('entities_black.list', encoding='utf8').read().splitlines()\n",
    "    white_list = io.open('entities_white.list', encoding='utf8').read().splitlines()\n",
    "    if (entity in map(str.lower, black_list)):\n",
    "        return False\n",
    "    elif ((len(entity) < 3) and (entity not in map(str.lower, white_list))):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def entities_to_interests(entities):\n",
    "    interests = {}\n",
    "    for entity in entities:\n",
    "        mentions = entity['mentions']\n",
    "        for mention in mentions:\n",
    "            interest = mention['text']['content']\n",
    "            counter = interests[interest] if interest in interests else 0\n",
    "            counter += 1\n",
    "            interests[interest] = counter\n",
    "    #interests = sorted(interests.items(), key=itemgetter(1), reverse=True)\n",
    "    interest_list = []\n",
    "    for interest in interests:\n",
    "        if relevant_entity(interest):\n",
    "            interest_list.append({'entity': interest, 'count': interests[interest]})\n",
    "    interest_list = sorted(interest_list, key=itemgetter('count'), reverse = True)\n",
    "    return interest_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones para trabajar con los intereses obtenidos de las entidades\n",
    "\n",
    "def path_interests(user):\n",
    "    return get_user_path(user).replace(\"tweets\", \"interests\")[:-1] + '.json'\n",
    "\n",
    "def interests_parsed(user):\n",
    "    files = glob(path_interests(user))\n",
    "    return len(files) > 0\n",
    "\n",
    "def save_interests(interests, user):\n",
    "    filepath = path_interests(user)\n",
    "    save_json(filepath, interests)\n",
    "\n",
    "def load_interests(user):\n",
    "    filepath = path_interests(user)\n",
    "    interests = read_json(filepath)\n",
    "    return interests\n",
    "\n",
    "def similar_interests(interest1, interest2):\n",
    "    interest1 = interest1.lower() # Ralentiza un pelin\n",
    "    interest2 = interest2.lower() # Ralentiza un pelin\n",
    "    interest1 = unidecode.unidecode(interest1) # Algo lento\n",
    "    interest2 = unidecode.unidecode(interest2) # Algo lento\n",
    "    proximity = SequenceMatcher(None, interest1, interest2).ratio() # Muy lento\n",
    "    return proximity\n",
    "\n",
    "def group_similar_interests(interests):\n",
    "    interests_group = []\n",
    "    for interest in interests:\n",
    "        max_similarity = 0\n",
    "        max_interest = None\n",
    "        for interest_group in interests_group:\n",
    "            similarity = similar_interests(interest['entity'], interest_group['entity'])\n",
    "            #print(similarity, interest['entity'].replace('\\n', ' '), '-' , interest_group['entity'].replace('\\n', ' '))\n",
    "            if (similarity > max_similarity):\n",
    "                #print(similarity, interest['entity'].replace('\\n', ' '), '-' , interest_group['entity'].replace('\\n', ' '))\n",
    "                max_similarity = similarity\n",
    "                max_interest = interest_group\n",
    "        if (max_similarity >= 0.75):\n",
    "            interest_grouped = deepcopy(max_interest)\n",
    "            interest_grouped['count'] = max_interest['count'] + interest['count']\n",
    "            interests_group.remove(max_interest)\n",
    "            interests_group.append(interest_grouped)\n",
    "            #print('+', max_similarity, interest['entity'].replace('\\n', ' '), '-' , max_interest['entity'].replace('\\n', ' '))\n",
    "        else:\n",
    "            interests_group.append(interest)\n",
    "    interests_group = sorted(interests_group, key=itemgetter('count'), reverse = True)\n",
    "    return interests_group\n",
    "\n",
    "def remove_low_interest(interests):\n",
    "    high_interests = []\n",
    "    for interest in interests:\n",
    "        if (interest['count'] > 1):\n",
    "            high_interests.append(interest)\n",
    "    return high_interests\n",
    "\n",
    "def normalize_weights(interests):\n",
    "    max_weight = interests[0]['count']\n",
    "    for interest in interests:\n",
    "        interest['weight'] = interest['count'] / max_weight\n",
    "    return interests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones para interactuar con la API de Google Cloud\n",
    "\n",
    "def gcloud_api():\n",
    "    service_account_info = get_config('gcloud')\n",
    "    credentials = service_account.Credentials.from_service_account_info(service_account_info)\n",
    "    client = language.LanguageServiceClient(credentials=credentials)\n",
    "    return client\n",
    "\n",
    "def extract_entities(text):\n",
    "    document = types.Document(content=text, type=enums.Document.Type.PLAIN_TEXT)\n",
    "    response = gcloud_api().analyze_entities(document)\n",
    "    return MessageToDict(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funciones de alto nivel para procesar las cuentas de los usuarios\n",
    "\n",
    "def summary_language_users():\n",
    "    users = list_users_obtained()\n",
    "    for user in users:\n",
    "        tweets = load_user_tweets(user)\n",
    "        tweets_total = count_user_tweets(user)\n",
    "        tweets_by_lang = group_by_language(tweets)\n",
    "        langs = [key for key in tweets_by_lang]\n",
    "        langs_list = []\n",
    "        for lang in langs:\n",
    "            percent = int(round(100*len(tweets_by_lang[lang])/tweets_total, 0))\n",
    "            language = {'lang': lang, 'percent': percent}\n",
    "            if (percent > 0): langs_list.append(language)\n",
    "        langs_list = sorted(langs_list, key=itemgetter('percent'), reverse = True)\n",
    "        langs = ['(%d%%) %s' % (language['percent'], language['lang']) for language in langs_list]\n",
    "        print('@' + user + ': ' + ', '.join(langs))\n",
    "\n",
    "def process_user_tweets(user):\n",
    "    if (not user_processed(user)):\n",
    "        tweets = load_user_tweets(user)\n",
    "        tweets_by_lang = group_by_language(tweets)\n",
    "        tweets_by_lang = reduce_languages(tweets_by_lang, ['es', 'en'])\n",
    "        for lang in ['es', 'en', 'others']:\n",
    "            try:\n",
    "                print('@' + user + ': processing ' + lang.upper() + '                     ', end='\\r')\n",
    "                tweets_lang = tweets_by_lang[lang] if (lang in tweets_by_lang) else []\n",
    "                entities_lang = extract_entities(compact_tweets(tweets_lang))\n",
    "                save_entities(entities_lang, user, lang)\n",
    "            except:\n",
    "                print('@' + user + ': bad language processing ' + lang.upper() + '        ', end='\\r')\n",
    "    if (not interests_parsed(user)):\n",
    "        print('@' + user + ': parsing interests                      ', end='\\r')\n",
    "        entities = load_entities(user)\n",
    "        interests = entities_to_interests(entities)\n",
    "        interests = group_similar_interests(interests)\n",
    "        interests = remove_low_interest(interests)\n",
    "        interests = normalize_weights(interests)\n",
    "        save_interests(interests, user)\n",
    "    else:\n",
    "        interests = load_interests(user)\n",
    "    density = round(len(interests) / count_user_tweets(user), 2)\n",
    "    print('@' + user + ': %d interests found (%.2f interests per tweet)' % (len(interests), density))\n",
    "\n",
    "def process_users_tweets():\n",
    "    users = list_users_obtained()\n",
    "    for user in users:\n",
    "        process_user_tweets(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_language_users() # Hacemos un resumen de los lenguajes que usa cada usuario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "process_users_tweets() # Procesamos los tuits de todos los usuarios recopilados\n",
    "\n",
    "print(time.time()-t0, 'seconds')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfg]",
   "language": "python",
   "name": "conda-env-tfg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
